{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting path variables and dataset size and number of abnormalities to be classified. Also change model_name to the name of the model that is being tested. The save_index is how often the model is going to be saved to the directory as a backup, save_index=n means the model is going to be saved every n epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path='/mnt/Velocity_Vault/ECG/Model/'\n",
    "memmap_path='/mnt/Velocity_Vault/ECG/Dataset/'\n",
    "\n",
    "disease_size=7\n",
    "dataset_size=19653\n",
    "\n",
    "save_index=3\n",
    "\n",
    "loss_threshold=0.003\n",
    "loss_counter_max=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset has not been splitted and is going to be entirely used for testing set org=True, if it has been splitted change org=False, also remember to change the respective dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org=False\n",
    "\n",
    "if org:\n",
    "    signal_name=memmap_path+'ecg_signal'\n",
    "    feature_name=memmap_path+'features'\n",
    "    label_name=memmap_path+'labels'\n",
    "else:\n",
    "    signal_name=memmap_path+'train_signal'\n",
    "    feature_name=memmap_path+'train_feat'\n",
    "    label_name=memmap_path+'train_labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_signal=np.memmap(signal_name, dtype='int16', mode='r', shape=(dataset_size, 12,5000))\n",
    "features=np.memmap(feature_name, dtype='float32', mode='r', shape=(dataset_size, 12))\n",
    "labels=np.memmap(label_name, dtype='int', mode='r', shape=(dataset_size, disease_size))\n",
    "\n",
    "\n",
    "ecg_signal = torch.tensor(ecg_signal, dtype=torch.float32)  # Convert to float32\n",
    "features = torch.tensor(features, dtype=torch.float32)      # Convert to float32\n",
    "labels = torch.tensor(labels, dtype=torch.float32)          # Convert to float32 for multi-label classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Noam optimizer (as explained earlier)\n",
    "class NoamOpt:\n",
    "    def __init__(self, model, warmup_steps, factor=1, optimizer=None):\n",
    "        self.model = model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.factor = factor\n",
    "        self.optimizer = optimizer if optimizer else optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "        self.step_num = 0\n",
    "\n",
    "    def rate(self):\n",
    "        \"\"\"Calculate the learning rate based on the Noam scheduler.\"\"\"\n",
    "        if self.step_num < self.warmup_steps:\n",
    "            return self.factor * (self.step_num + 1) / self.warmup_steps\n",
    "        else:\n",
    "            return self.factor * (self.step_num + 1) ** -0.5\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Update the model's parameters.\"\"\"\n",
    "        self.step_num += 1\n",
    "        lr = self.rate()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Clear gradients for the optimizer.\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "# Define your model architecture (CTN model and Transformer as already defined)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].clone().detach().to(dtype=torch.float32, device=x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, num_layers, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = nhead\n",
    "        self.d_ff = d_ff\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Add dropout to the encoder layers\n",
    "        self.pe = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # Transformer Encoder Layer\n",
    "        encode_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model, \n",
    "            nhead=self.h, \n",
    "            dim_feedforward=self.d_ff, \n",
    "            dropout=dropout,\n",
    "            batch_first=True \n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encode_layer, self.num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Permute dimensions to match Transformer expectations: (batch_size, seq_len, d_model)\n",
    "        out = x.permute(0, 2, 1)\n",
    "        out = self.pe(out)  # Add positional encoding\n",
    "        out = out.permute(1, 0, 2)  # Permute back for transformer encoder\n",
    "        out = self.transformer_encoder(out)\n",
    "        out = out.mean(0)  # Global average pooling over sequence length\n",
    "        return out\n",
    "\n",
    "\n",
    "class CTN(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, num_layers, dropout_rate, deepfeat_sz, nb_feats, classes):\n",
    "        super(CTN, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(12, 128, kernel_size=14, stride=3, padding=2, bias=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(128, 256, kernel_size=14, stride=3, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(256, d_model, kernel_size=10, stride=2, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=10, stride=2, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=10, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=10, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.transformer = Transformer(d_model, nhead, d_ff, num_layers, dropout=0.1)\n",
    "        self.fc1 = nn.Linear(d_model, deepfeat_sz)\n",
    "        self.fc2 = nn.Linear(deepfeat_sz + nb_feats, len(classes))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, wide_feats):\n",
    "        z = self.encoder(x)  # Encoded sequence\n",
    "        out = self.transformer(z)  # Transformer output\n",
    "        out = self.dropout(F.relu(self.fc1(out)))\n",
    "        out = self.fc2(torch.cat([wide_feats, out], dim=1))\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model. Change the number of epochs to desired size (50 works good in general). If too much resources are being used reduce the batch size in the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 8.0942, Accuracy: 53.35%\n",
      "Epoch 2/50, Loss: 2.2865, Accuracy: 70.20%\n",
      "Epoch 3/50, Loss: 0.4602, Accuracy: 82.12%\n",
      "Model saved as model_epoch_3.pth\n",
      "Epoch 4/50, Loss: 0.3343, Accuracy: 86.71%\n",
      "Epoch 5/50, Loss: 0.2776, Accuracy: 88.85%\n",
      "Epoch 6/50, Loss: 0.2547, Accuracy: 89.74%\n",
      "Model saved as model_epoch_6.pth\n",
      "Epoch 7/50, Loss: 0.2400, Accuracy: 90.41%\n",
      "Epoch 8/50, Loss: 0.2306, Accuracy: 90.73%\n",
      "Epoch 9/50, Loss: 0.2197, Accuracy: 91.20%\n",
      "Model saved as model_epoch_9.pth\n",
      "Epoch 10/50, Loss: 0.2143, Accuracy: 91.41%\n",
      "Epoch 11/50, Loss: 0.2072, Accuracy: 91.70%\n",
      "Epoch 12/50, Loss: 0.2013, Accuracy: 91.98%\n",
      "Model saved as model_epoch_12.pth\n",
      "Epoch 13/50, Loss: 0.1939, Accuracy: 92.29%\n",
      "Epoch 14/50, Loss: 0.1887, Accuracy: 92.48%\n",
      "Epoch 15/50, Loss: 0.1826, Accuracy: 92.85%\n",
      "Model saved as model_epoch_15.pth\n",
      "Epoch 16/50, Loss: 0.1765, Accuracy: 93.11%\n",
      "Epoch 17/50, Loss: 0.1697, Accuracy: 93.30%\n",
      "Epoch 18/50, Loss: 0.1643, Accuracy: 93.56%\n",
      "Model saved as model_epoch_18.pth\n",
      "Epoch 19/50, Loss: 0.1561, Accuracy: 93.91%\n",
      "Epoch 20/50, Loss: 0.1497, Accuracy: 94.23%\n",
      "Epoch 21/50, Loss: 0.1445, Accuracy: 94.42%\n",
      "Model saved as model_epoch_21.pth\n",
      "Epoch 22/50, Loss: 0.1348, Accuracy: 94.81%\n",
      "Epoch 23/50, Loss: 0.1291, Accuracy: 95.11%\n",
      "Epoch 24/50, Loss: 0.1211, Accuracy: 95.38%\n",
      "Model saved as model_epoch_24.pth\n",
      "Epoch 25/50, Loss: 0.1137, Accuracy: 95.64%\n",
      "Epoch 26/50, Loss: 0.1069, Accuracy: 95.92%\n",
      "Epoch 27/50, Loss: 0.0989, Accuracy: 96.34%\n",
      "Model saved as model_epoch_27.pth\n",
      "Epoch 28/50, Loss: 0.0918, Accuracy: 96.56%\n",
      "Epoch 29/50, Loss: 0.0863, Accuracy: 96.77%\n",
      "Epoch 30/50, Loss: 0.0824, Accuracy: 96.95%\n",
      "Model saved as model_epoch_30.pth\n",
      "Epoch 31/50, Loss: 0.0762, Accuracy: 97.15%\n",
      "Epoch 32/50, Loss: 0.0715, Accuracy: 97.33%\n",
      "Epoch 33/50, Loss: 0.0677, Accuracy: 97.50%\n",
      "Model saved as model_epoch_33.pth\n",
      "Epoch 34/50, Loss: 0.0634, Accuracy: 97.66%\n",
      "Epoch 35/50, Loss: 0.0586, Accuracy: 97.87%\n",
      "Epoch 36/50, Loss: 0.0554, Accuracy: 97.97%\n",
      "Model saved as model_epoch_36.pth\n",
      "Epoch 37/50, Loss: 0.0528, Accuracy: 98.07%\n",
      "Epoch 38/50, Loss: 0.0501, Accuracy: 98.17%\n",
      "Epoch 39/50, Loss: 0.0456, Accuracy: 98.35%\n",
      "Model saved as model_epoch_39.pth\n",
      "Epoch 40/50, Loss: 0.0430, Accuracy: 98.44%\n",
      "Epoch 41/50, Loss: 0.0422, Accuracy: 98.51%\n",
      "Epoch 42/50, Loss: 0.0421, Accuracy: 98.45%\n",
      "Model saved as model_epoch_42.pth\n",
      "Epoch 43/50, Loss: 0.0381, Accuracy: 98.60%\n",
      "Epoch 44/50, Loss: 0.0388, Accuracy: 98.61%\n",
      "Epoch 45/50, Loss: 0.0350, Accuracy: 98.74%\n",
      "Model saved as model_epoch_45.pth\n",
      "Epoch 46/50, Loss: 0.0350, Accuracy: 98.74%\n",
      "Epoch 47/50, Loss: 0.0333, Accuracy: 98.81%\n",
      "Epoch 48/50, Loss: 0.0329, Accuracy: 98.81%\n",
      "Model saved as model_epoch_48.pth\n",
      "Epoch 49/50, Loss: 0.0301, Accuracy: 98.93%\n",
      "Threshold reached\n",
      "Final Model Saved as final_model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DataLoader for training\n",
    "train_dataset = TensorDataset(ecg_signal, features, labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 256\n",
    "nhead = 8\n",
    "d_ff = 512\n",
    "num_layers = 8\n",
    "dropout_rate = 0.1\n",
    "deepfeat_sz = 64\n",
    "nb_feats = 12  # Update this to 12 to reflect the wide features\n",
    "classes = [i for i in range(disease_size)] # 5 classes\n",
    "warmup_steps = 1000  # Number of warm-up steps for the Noam optimizer\n",
    "epochs = 50  # Number of training epochs\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = CTN(d_model=d_model, nhead=nhead, d_ff=d_ff, num_layers=num_layers,\n",
    "            dropout_rate=dropout_rate, deepfeat_sz=deepfeat_sz,\n",
    "            nb_feats=nb_feats, classes=classes)\n",
    "\n",
    "# Create Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0)\n",
    "\n",
    "# Use Noam optimizer\n",
    "noam_optimizer = NoamOpt(model, warmup_steps, optimizer=optimizer)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "\n",
    "prev_loss=0\n",
    "loss_counter=0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        ecg_signal, wide_feats, labels = batch\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(ecg_signal, wide_feats)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()  # Backpropagation\n",
    "        \n",
    "        # Step the Noam optimizer (updates learning rate and performs optimization)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predicted = torch.sigmoid(outputs) > 0.5\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.numel()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = correct_preds / total_preds * 100\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "        # Save the model after every 5 epochs\n",
    "    if (epoch + 1) % save_index == 0:\n",
    "        model_save_path = os.path.join(model_path, f\"model_epoch_{epoch+1}.pth\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved as model_epoch_{epoch+1}.pth\")\n",
    "        \n",
    "    if (epoch + 1)==epochs:\n",
    "        model_save_path = os.path.join(model_path, \"final_model.pth\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(\"Final Model Saved as final_model.pth\")\n",
    "        \n",
    "    #print(avg)\n",
    "    if abs(avg_loss-prev_loss)<loss_threshold:\n",
    "        loss_counter+=1\n",
    "    else:\n",
    "        loss_counter=0\n",
    "        \n",
    "    prev_loss=avg_loss\n",
    "        \n",
    "    if loss_counter==loss_counter_max:\n",
    "        print('Threshold reached')\n",
    "        model_save_path = os.path.join(model_path, \"final_model.pth\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(\"Final Model Saved as final_model.pth\")\n",
    "        break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
