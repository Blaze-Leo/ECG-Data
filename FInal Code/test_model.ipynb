{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting path variables and dataset size and number of abnormalities to be classified. Also change model_name to the name of the model that is being tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path='/mnt/Velocity_Vault/ECG/Model/'\n",
    "memmap_path='/mnt/Velocity_Vault/ECG/Dataset/'\n",
    "\n",
    "model_name=\"final_model.pth\"\n",
    "\n",
    "disease_size=7\n",
    "dataset_size=2184"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset has not been splitted and is going to be entirely used for testing set org=True, if it has been splitted change org=False, also remember to change the respective dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "org=False\n",
    "\n",
    "if org:\n",
    "    signal_name=memmap_path+'ecg_signal'\n",
    "    feature_name=memmap_path+'features'\n",
    "    label_name=memmap_path+'labels'\n",
    "else:\n",
    "    signal_name=memmap_path+'test_signal'\n",
    "    feature_name=memmap_path+'test_feat'\n",
    "    label_name=memmap_path+'test_labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_signal=np.memmap(signal_name, dtype='int16', mode='r', shape=(dataset_size, 12,5000))\n",
    "features=np.memmap(feature_name, dtype='float32', mode='r', shape=(dataset_size, 12))\n",
    "ground_labels=np.memmap(label_name, dtype='int', mode='r', shape=(dataset_size, disease_size))\n",
    "\n",
    "# Convert np.memmap arrays to PyTorch tensors\n",
    "ecg_signal = torch.tensor(ecg_signal, dtype=torch.float32)  # Convert to float32\n",
    "features = torch.tensor(features, dtype=torch.float32)      # Convert to float32\n",
    "labels = torch.tensor(ground_labels, dtype=torch.float32)          # Convert to float32 for multi-label classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Noam optimizer (as explained earlier)\n",
    "class NoamOpt:\n",
    "    def __init__(self, model, warmup_steps, factor=1, optimizer=None):\n",
    "        self.model = model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.factor = factor\n",
    "        self.optimizer = optimizer if optimizer else optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "        self.step_num = 0\n",
    "\n",
    "    def rate(self):\n",
    "        \"\"\"Calculate the learning rate based on the Noam scheduler.\"\"\"\n",
    "        if self.step_num < self.warmup_steps:\n",
    "            return self.factor * (self.step_num + 1) / self.warmup_steps\n",
    "        else:\n",
    "            return self.factor * (self.step_num + 1) ** -0.5\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Update the model's parameters.\"\"\"\n",
    "        self.step_num += 1\n",
    "        lr = self.rate()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Clear gradients for the optimizer.\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "# Define your model architecture (CTN model and Transformer as already defined)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        x = x + self.pe[:, :x.size(1)].clone().detach().to(dtype=torch.float32, device=x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, num_layers, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = nhead\n",
    "        self.d_ff = d_ff\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Add dropout to the encoder layers\n",
    "        self.pe = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # Transformer Encoder Layer\n",
    "        encode_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model, \n",
    "            nhead=self.h, \n",
    "            dim_feedforward=self.d_ff, \n",
    "            dropout=dropout,\n",
    "            batch_first=True \n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encode_layer, self.num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Permute dimensions to match Transformer expectations: (batch_size, seq_len, d_model)\n",
    "        out = x.permute(0, 2, 1)\n",
    "        out = self.pe(out)  # Add positional encoding\n",
    "        out = out.permute(1, 0, 2)  # Permute back for transformer encoder\n",
    "        out = self.transformer_encoder(out)\n",
    "        out = out.mean(0)  # Global average pooling over sequence length\n",
    "        return out\n",
    "\n",
    "\n",
    "class CTN(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, num_layers, dropout_rate, deepfeat_sz, nb_feats, classes):\n",
    "        super(CTN, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(12, 128, kernel_size=14, stride=3, padding=2, bias=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(128, 256, kernel_size=14, stride=3, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(256, d_model, kernel_size=10, stride=2, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=10, stride=2, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=10, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=10, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.transformer = Transformer(d_model, nhead, d_ff, num_layers, dropout=0.1)\n",
    "        self.fc1 = nn.Linear(d_model, deepfeat_sz)\n",
    "        self.fc2 = nn.Linear(deepfeat_sz + nb_feats, len(classes))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, wide_feats):\n",
    "        z = self.encoder(x)  # Encoded sequence\n",
    "        out = self.transformer(z)  # Transformer output\n",
    "        out = self.dropout(F.relu(self.fc1(out)))\n",
    "        out = self.fc2(torch.cat([wide_feats, out], dim=1))\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "CTN(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv1d(12, 128, kernel_size=(14,), stride=(3,), padding=(2,), bias=False)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv1d(128, 256, kernel_size=(14,), stride=(3,), bias=False)\n",
       "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv1d(256, 256, kernel_size=(10,), stride=(2,), bias=False)\n",
       "    (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv1d(256, 256, kernel_size=(10,), stride=(2,), bias=False)\n",
       "    (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv1d(256, 256, kernel_size=(10,), stride=(1,), bias=False)\n",
       "    (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): Conv1d(256, 256, kernel_size=(10,), stride=(1,), bias=False)\n",
       "    (16): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (pe): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=76, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Initialize the model again (same as during training)\n",
    "d_model = 256\n",
    "nhead = 8\n",
    "d_ff = 512\n",
    "num_layers = 8\n",
    "dropout_rate = 0.1\n",
    "deepfeat_sz = 64\n",
    "nb_feats = 12\n",
    "classes = [0, 1, 2, 3, 4,5,6]  # 5 classes\n",
    "model = CTN(d_model=d_model, nhead=nhead, d_ff=d_ff, num_layers=num_layers,\n",
    "            dropout_rate=dropout_rate, deepfeat_sz=deepfeat_sz,\n",
    "            nb_feats=nb_feats, classes=classes)\n",
    "\n",
    "# 2. Load the saved model weights (make sure to specify the correct path)\n",
    "model_save_path = model_path+model_name  # Example: Load the model saved at epoch 5\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# 3. Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are made and sigmoid functions are converted into boolean values so that they can be compared to the ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:21<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TensorDataset(ecg_signal, features, labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True,num_workers=0)\n",
    "\n",
    "\n",
    "# 6. Testing the model\n",
    "predictions = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients during testing\n",
    "    for batch in tqdm(test_loader):\n",
    "        ecg_signal, wide_feats, labels = batch\n",
    "        outputs = model(ecg_signal, wide_feats)\n",
    "        \n",
    "        # Store the predictions and ground truth\n",
    "        predictions.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "        labels_list.append(labels.cpu().numpy())\n",
    "\n",
    "# Convert predictions and labels to numpy arrays\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "labels_list = np.concatenate(labels_list, axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2184, 7)\n",
      "(2184, 7)\n"
     ]
    }
   ],
   "source": [
    "pred_labels=np.array(labels_list,dtype=int)\n",
    "\n",
    "pprint(ground_labels.shape)\n",
    "pprint(pred_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of ROC AUC score values:\n",
    "\n",
    "    0.1 - 0.5 → Worse than Random Guessing\n",
    "    0.5 → Random guessing (no discrimination)\n",
    "    0.7 - 0.8 → Fair model\n",
    "    0.8 - 0.9 → Good model\n",
    "    0.9 - 1.0 → Excellent model\n",
    "    1.0 → Perfect model (likely overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.9098\n"
     ]
    }
   ],
   "source": [
    "# 7. Calculate ROC AUC score for multi-label classification\n",
    "roc_auc = roc_auc_score(labels_list, predictions, average='macro', multi_class='ovr')\n",
    "print(f\"Test ROC AUC: {roc_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
